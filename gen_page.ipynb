{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is for generating the page interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from os.path import expanduser\n",
    "from os import system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates.audio_table import table_with_texts\n",
    "from templates.frame import header, footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_map = {\n",
    "    \"nsf_adv\": \"NSF-adv\",\n",
    "    \"nsf_adv_tts\": \"Tacotron2 + NSF-adv\",\n",
    "    \"pwg\": \"Parallel WaveGAN\",\n",
    "    \"pwg_tts\": \"Tacotron2 + Parallel WaveGAN\",\n",
    "    \"h_sinc_nsf\": \"hn-sinc-NSF\",\n",
    "    \"h_sinc_nsf_tts\": \"Tacotron2 + hn-sinc-NSF\",\n",
    "    \"nhv_noadv\": \"NHV-noadv\",\n",
    "    \"original\": \"Original\",\n",
    "    \"nhv\": \"NHV(cGAN)\",\n",
    "    \"nhv_non_conditional_gan\": \"NHV(GAN)\",\n",
    "    \"nhv_tts\": \"Tacotron2 + NHV\",\n",
    "    \"wavenet\": \"MoL WaveNet\",\n",
    "    \"ddsp\": \"DDSP(Sinusoids + Noise)\",\n",
    "    \"spn_adv\": \"DDSP(S+N, cGAN)\"\n",
    "}\n",
    "file_root = Path(\"/home/sorcerer/Experiments/samples/\")\n",
    "target_root = Path(\"samples/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_id = [9913, 9926, 9933, 9963, 9977, 9979, 9988]\n",
    "# Cherry picked for sentences with good prosody. \n",
    "tts_id = [9917, 9918, 9920, 9925, 9929, 9942, 9957]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_model = [\"original\", \"nhv_non_conditional_gan\", \"nhv\", \"pwg\", \"nsf_adv\", \"wavenet\", \"h_sinc_nsf\", \"nhv_noadv\", \"ddsp\", \"spn_adv\"]\n",
    "tts_model = [\"original\", \"nhv_tts\", \"pwg_tts\", \"nsf_adv_tts\", \"h_sinc_nsf_tts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNTextLoader:\n",
    "    def __init__(self, text_file_path):\n",
    "        text_file_path = Path(expanduser(str(text_file_path)))\n",
    "        assert text_file_path.exists(), \"Text File Not Found\"\n",
    "        with open(text_file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        self.idxs = idxs = [int(l.strip().split()[0]) for l in lines[::2]]\n",
    "        self.texts = texts = [l.strip().split()[1] for l in lines[::2]]\n",
    "        self.pinyins = pinyins = [l.strip().split() for l in lines[1::2]]\n",
    "        self.pinyins[2364] = ['zhe4', 'tu2', 'nan2', 'bu4', 'cheng2', 'shi4', 'pi1', 'guo4', 'de5']\n",
    "        self.idx_text = {}\n",
    "        self.idx_pinyin = {}\n",
    "        for idx, text, pinyin in zip(idxs, texts, pinyins):\n",
    "            self.idx_text[idx] = text\n",
    "            self.idx_pinyin[idx] = pinyin\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.idx_text[idx], self.idx_pinyin[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_text_loader = CNTextLoader(\"/home/sorcerer/datasets/BZNSYP/ProsodyLabeling/000001-010000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_text = [cn_text_loader[idx][0] for idx in copy_id]\n",
    "tts_text  = [cn_text_loader[idx][0] for idx in tts_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "system(\"rm -rf samples/\")\n",
    "system(\"mkdir ./samples\")\n",
    "for model_id in code_map:\n",
    "    system(\"mkdir ./samples/\" + model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_audios = []\n",
    "copy_titles = []\n",
    "for i in copy_id:\n",
    "    audios = []\n",
    "    titles = []\n",
    "    for model_id in copy_model:\n",
    "        source_file = file_root / model_id / f\"{i:06d}.wav\"\n",
    "        target_file = target_root / model_id / f\"{i:06d}.wav\"\n",
    "        system(f\"cp {str(source_file)} {str(target_file)}\")\n",
    "        audios.append(\"/nhv-web/\" + str(target_file))\n",
    "        titles.append(code_map[model_id])\n",
    "    copy_audios.append(audios)\n",
    "    copy_titles.append(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_audios = []\n",
    "tts_titles = []\n",
    "for i in tts_id:\n",
    "    audios = []\n",
    "    titles = []\n",
    "    for model_id in tts_model:\n",
    "        source_file = file_root / model_id / f\"{i:06d}.wav\"\n",
    "        target_file = target_root / model_id / f\"{i:06d}.wav\"\n",
    "        system(f\"cp {str(source_file)} {str(target_file)}\")\n",
    "        audios.append(\"/nhv-web/\" + str(target_file))\n",
    "        titles.append(code_map[model_id])\n",
    "    tts_audios.append(audios)\n",
    "    tts_titles.append(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = header(\"Online Supplement\") + \\\n",
    "    \"\"\"\n",
    "    <div class=\"page-header\">\n",
    "        <h1>Neural Homomorphic Vocoder <small>Online supplement for InterSpeech 2020</small></h1>\n",
    "    </div>\n",
    "    <h3> Authors </h3>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-md-4\">\n",
    "            <address>\n",
    "                <strong>Zhijun Liu</strong><br>\n",
    "                <a href=\"#\">sorcerer~at~sjtu.edu.cn</a>\n",
    "            </address>\n",
    "        </div>\n",
    "        <div class=\"col-md-4\">\n",
    "            <address>\n",
    "                <strong>Kuan Chen</strong><br>\n",
    "                <a href=\"#\">azrealkuan~at~sjtu.edu.cn</a>\n",
    "            </address>\n",
    "        </div>\n",
    "        <div class=\"col-md-4\">\n",
    "            <address>\n",
    "                <strong>Kai Yu</strong><br>\n",
    "                <a href=\"#\">kai.yu~at~sjtu.edu.cn</a>\n",
    "            </address>\n",
    "        </div>\n",
    "    </div>\n",
    "    <ul>\n",
    "        <li>\n",
    "            <strong>\n",
    "            <a href=\"/nhv-web/phase.html\">This jupyter notebook</a> demonstrates the importance of phase in speech.\n",
    "            </strong>\n",
    "        </li>\n",
    "        <li>\n",
    "        The <a href=\"https://www.data-baker.com/open_source.html\">Chinese Standard Mandarin Speech Copus</a> is used for the demo. \n",
    "        </li>\n",
    "        <li>\n",
    "        All <strong>generated samples</strong> can be found on the <a href=\"https://drive.google.com/drive/folders/1IY28v3kHZh6e12wB0y0p2IXjqI9vwmH9?usp=sharing\">google drive</a>.\n",
    "        </li>\n",
    "        <li>\n",
    "        The analysis of <strong>computational complexity</strong> can be found in <a href=\"/nhv-web/computational_complexity.html\">this jupyter notebook</a>.\n",
    "        </li>\n",
    "        <li>\n",
    "        Further <strong>training details</strong> can be found in <a href=\"/nhv-web/training_details.html\">this jupyter notebook</a>.\n",
    "        </li>\n",
    "    </ul>\n",
    "    <h3> Copy Synthesis Demos </h3>\n",
    "    <div class=\"alert alert-info\" role=\"alert\">\n",
    "        <p>\n",
    "        NHV(cGAN) is the model described in the paper and used in evaluation. NHV(GAN) was trained with the same discriminator used in b-NSF-adv. They have slightly different loss in audio quality. \n",
    "        </p>\n",
    "        <p>\n",
    "        DDSP(Sinusoids + Noise) uses the same network structure and inputs as NHV models. The output of the networks are replaced with `harmonic distributions` and `noise filter FFT amplitudes` as described in the DDSP paper. The F0 is encoded with 80 dimensional one-hot vectors in log-scale.\n",
    "        This model was not used in evaluations.\n",
    "        </p>\n",
    "        <p>\n",
    "        DDSP(S+N, cGAN) is the same as DDSP(S+N). Except that it is trained with cGAN. The GAN structure and hyper-parameters are the same as that used in NHV(cGAN).\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\" + \\\n",
    "    table_with_texts(\n",
    "        copy_text,\n",
    "        copy_audios,\n",
    "        copy_titles,\n",
    "        width=3\n",
    "    ) + \\\n",
    "    \"\"\"<h3> Text-to-Speech Demos </h3>\"\"\" + \\\n",
    "    table_with_texts(\n",
    "        tts_text,\n",
    "        tts_audios,\n",
    "        tts_titles,\n",
    "        width=3\n",
    "    ) + \\\n",
    "    footer()\n",
    "with open(\"index.html\", \"w\") as f:\n",
    "    f.write(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /home/sorcerer/Experiments/2020/Tacotron+NHV/Final/phase.ipynb phase.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mConverting computational_complexity.ipynb to HTML.\u001b[32m\n",
      "[NbConvertApp] Converting notebook computational_complexity.ipynb to html\n",
      "[NbConvertApp] Writing 297898 bytes to computational_complexity.html\n",
      "\u001b[34mConverting phase.ipynb to HTML.\u001b[32m\n",
      "[NbConvertApp] Converting notebook phase.ipynb to html\n",
      "[NbConvertApp] Writing 1680105 bytes to phase.html\n",
      "\u001b[34mConverting training_details.ipynb to HTML.\u001b[32m\n",
      "[NbConvertApp] Converting notebook training_details.ipynb to html\n",
      "[NbConvertApp] Writing 297915 bytes to training_details.html\n"
     ]
    }
   ],
   "source": [
    "!source ./convert_all_ipynb_to_html.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
