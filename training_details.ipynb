{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Details of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Mel Spectrogram Analysis\n",
    "analysis_stft_fft_length = 2048\n",
    "analysis_stft_window_length = 512\n",
    "mel_length = 80\n",
    "mel_min_f0 = 40.0\n",
    "mel_max_f0 = 7600.0\n",
    "\n",
    "#### Sampling\n",
    "sampling_rate = 22050\n",
    "frame_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F0 Extraction\n",
    "\n",
    "Many thanks to the authors of python wrappers, and the open source speech processing tools.\n",
    "  - WORLD wrapper: https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder\n",
    "  - REAPER wrapper: https://github.com/r9y9/pyreaper\n",
    "  - WORLD vocoder: https://github.com/mmorise/World\n",
    "  - REAPER: https://github.com/google/REAPER\n",
    "\n",
    "We used the following function to extract f0: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyworld import dio, stonemask\n",
    "from pyreaper import reaper\n",
    "\n",
    "\n",
    "def repaer_stonemask(double_x, frame_length, sampling_rate):\n",
    "    \"\"\"\n",
    "    double_x: numpy double array [Samples]\n",
    "    frame_length: int, # of samples in a single frame\n",
    "    sampling_rate: int\n",
    "    returns: numpy double array [Frames]\n",
    "    \"\"\"\n",
    "    n_frames = len(double_x) // frame_length\n",
    "    n_samples = n_frames * frame_length\n",
    "    double_x = double_x[:n_samples]\n",
    "    int_x = np.clip(double_x * (65536 // 2), -32768, 32767).astype(np.int16)\n",
    "    times = np.linspace(0, n_frames - 1, n_frames) * frame_length / sampling_rate + frame_length / 2 / sampling_rate\n",
    "    _, _, f0_times, f0, _ = reaper(int_x, sampling_rate, minf0=40.0, maxf0=600.0)\n",
    "    coarse_f0 = np.interp(times, f0_times, f0)\n",
    "    fine_f0 = stonemask(double_x, coarse_f0, times, sampling_rate)\n",
    "    return fine_f0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "We used the Adam Optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon=1\\times 10^{-8}$ in all models.\n",
    "We applied Weight Normalization to all network weights.\n",
    "In all experiments we used the following Noam learning rate scheduling, with different learning rates for generators and discriminators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noam_lr(warmup_steps, min_lr, init_lr, step, power=0.35):\n",
    "    return np.maximum(\n",
    "        init_lr * warmup_steps ** power * np.minimum(\n",
    "            step * warmup_steps ** (-1 - power), \n",
    "            step ** -power\n",
    "        ), \n",
    "        min_lr\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses\n",
    "\n",
    "The weight between adversarial loss $L_G$, $L_D$, and reconstruction loss $L_R$ is different for each model. See `adv_ratio` in configs below. \n",
    "\n",
    "We used following STFTLoss Definition in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFTLoss(nn.Module):\n",
    "    def __init__(self, fft_lengths, window_lengths, hop_lengths, loss_scale_type):\n",
    "        \"\"\"\n",
    "        STFT Loss\n",
    "        fft_lengths: list of int\n",
    "        window_lengths: list of int\n",
    "        hop_lengths: list of int\n",
    "        loss_scale_type: str defining the scale of loss\n",
    "        \"\"\"\n",
    "        super(STFTLoss, self).__init__()\n",
    "        self.fft_lengths = fft_lengths\n",
    "        self.window_lengths = window_lengths\n",
    "        self.hop_lengths = hop_lengths\n",
    "        self.loss_scale_type = loss_scale_type\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x: FloatTensor [Batch, 1, T]\n",
    "        y: FloatTensor [Batch, 1, T]\n",
    "        returns: FloatTensor [] as total loss\n",
    "        \"\"\"\n",
    "        x, y = x.squeeze(1), y.squeeze(1)\n",
    "        loss = 0.0\n",
    "        batch_size = x.size(0)\n",
    "        z = torch.cat([x, y], dim=0) # [2 x Batch, T]\n",
    "        for fft_length, window_length, hop_length in zip(self.fft_lengths, \\\n",
    "            self.window_lengths, self.hop_lengths):\n",
    "            window = torch.hann_window(window_length)\n",
    "            Z = torch.stft(z, fft_length, hop_length, window_length, window) # [2 x Batch, Frame, 2]\n",
    "            SquareZ = Z.pow(2).sum(dim=-1) + 1e-10 # [2 x Batch, Frame]\n",
    "            SquareX, SquareY = SquareZ.split(batch_size, dim=0)\n",
    "            MagZ = SquareZ.sqrt()\n",
    "            MagX, MagY = MagZ.split(batch_size, dim=0)\n",
    "            if self.loss_scale_type == \"log_linear\":\n",
    "                loss += (MagX - MagY).abs().mean() + 0.5 * (SquareX.log() - SquareY.log()).abs().mean()\n",
    "            elif self.loss_scale_type == \"linear\":\n",
    "                loss += (MagX - MagY).abs().mean()\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unrecognized loss scale type {self.loss_scale_type}\")\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Hyper-Parameters\n",
    "  - We used [the source code of NSF](https://github.com/nii-yamagishilab/project-CURRENNT-scripts). We only changed the sampling rate config. `hn-sinc-NSF` used in evaluation is trained for 49 epoches. Training took about 3 days on a single 2080Ti.\n",
    "  - NSF-noadv is trained without adversarial loss functions. Training took less than 1 day on a single 2080Ti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Models                               | NHV(cGAN)                                                    | NHV(GAN)                                                     | b-NSF-adv                                                    | Parallel WaveGAN                                             | DDSP(S+N)                                                    | DDSP(S+N, cGAN)        |\n",
    "| :----------------------------------- | :----------------------------------------------------------- | ------------------------------------------------------------ | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- | :---------------------- |\n",
    "| **G Learning Rate**                  | Noam LR Scheduling,<br />warm up steps = 4000,<br />initial LR = 0.0006,<br />min LR = 0.00001 | Same as NHV.                                                 | Same as NHV;<br />except that initial LR is 0.001            | Same as b-NSF-adv.                                           | Same as NHV.                                                 | Same as NHV.           |\n",
    "| **D Learning Rate**                  | Noam LR Scheduling,<br />warm up steps = 20000,<br />initial LR = 0.0002,<br />min LR = 0.00001 | Same as NHV.                                                 | Same as NHV.                                                 | Same as NHV.                                                 | N/A                                                          | Same as NHV.           |\n",
    "| **Optimizers**                       | Default Adam in PyTorch. Adam with $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$, $\\epsilon = 1\\times 10 ^{-8}$ | Same as NHV.                                                 | Same as NHV.                                                 | Same as NHV.                                                 | Same as NHV.                                                 | Same as NHV.           |\n",
    "| **Reconstruction Loss $L_R$**        | STFT Loss with window sizes (128, 256, 384, 512, 640, 768, 896, 1024, 1536, 2048, 3072, 4096), <br />sum of L1 and log-L1 amplitude distance, (`log_linear` in the implementation); <br />FFT lengths are twice the window sizes; Window shifts are 1/4 the window sizes; | Same as NHV.                                                 | Same as NHV; window lengths is changed to (16, 32, 64, 128, 256, 512, 1024, 2048); <br />We used L1 amplitude distance only. | Same as b-NSF-adv.                                           | Same as NHV.                                                 | Same as NHV.           |\n",
    "| **Weight of $L_R$ and $L_G$, $L_D$** | $L_G$ and $L_D$ weighted by 4.0                              | Same as NHV.                                                 | $L_G$ and $L_D$ weighted by 0.1                              | $L_G$ and $L_D$ weighted by 0.4                              | N/A                                                          | Same as NHV.           |\n",
    "| **Generator Details**                | Described in our paper. Conditioned on only log-mel spectrogram. The final FIR has length 1000, and an exponential decaying rate 0.995. | Same as NHV.                                                 | Same as described in the original paper. Conditioned on only log-mel spectrogram. | Same as described in the original paper. Non-causal 30 layer WaveNet Conditioned on log f0, VUV and log-mel spectrograms. | Network same as NHV models. The parameters for harmonic and noise components  are harmonic distributions and noise filter FFT amplitude. Output  scaled by $\\exp(\\cdot)$. Harmonic distributions upsampled by hann windows as described in the  DDSP paper. | Same as DDSP(S+N).     |\n",
    "| **Discriminator Details**            | A Non-causal WaveNet  conditioned on log-mel spectrogram.    | A 10 layer CNN, with kernel size set to 3, strides set to (2 ,2, 4, 2, 2, 2, 1, 1, 1, 1). Each layer followed by leaky ReLU activation with negative slope 0.2. No conditioning used. | Same as NHV(GAN)                                             | Same as described in the original paper. A 10 layer CNN, with kernel size set to 3, dilations set to (1, 1, 2, 3, 4, 5, 6, 7, 8, 1). Each layer followed by leaky RELU activation with negative slope 0.2. No conditioning used. | N/A                                                          | Same as NHV.           |\n",
    "| **Condition Upsampling**             | N/A                                                          | N/A                                                          | Basically the same as described in the original paper. BiLSTM + 1DConv, both with channels = 80. Kernel size of CNN is 3. CNN is followed by BatchNorm1D and $\\tanh$ activation. | Upsampled by two  ConvTranspose2D. First layer has kernel size (3, 16) with strides (1, 8). The second layer has kernel size (3, 32) with strides (1, 16). So the time dimension is upscaled by a factor of 128. Each layer is followed by a LeakyReLU activation with negative slope 0.4 | N/A                                                          | N/A                    |\n",
    "| **F0 Upsampling**                    | Repeat                                                       | Repeat                                                       | Repeat                                                       | N/A                                                          | Linear Interpolation                                         | Linear Interpolation   |\n",
    "| **Adversarial Loss Formulation**     | Hinge version GAN objective.                                 | Same as NHV.                                                 | Same as NHV.                                                 | Same as NHV.                                                 | N/A                                                          | Same as NHV.           |\n",
    "| **Total Steps**                      | 350K                                                         | 350K                                                         | 350K                                                         | 400K                                                         | 400K                                                         | 400K                   |\n",
    "| **Training Time**                    | About 2 days on a single 2080Ti                              | About 1.5 days on 2080Ti                                     | About 2 days on 2080 Ti                                      | About 2 days on 2080 Ti                                      | Less than 1 day on 1080Ti                                    | About 2 days on 2080Ti |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
